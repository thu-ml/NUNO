from timeit import default_timer
from tqdm import tqdm
from util.utilities import *
from torch.optim import Adam
from .nufno2d import NUFNO2d
from src.kdtree.tree import KDTree
from sklearn.neighbors import KernelDensity


################################################################
# Configs
################################################################
# Data size
ntrain = 1000
ntest = 200
ntotal = ntrain + ntest
# Data paths
PATH = 'data/elasticity/'
PATH_SIGMA = PATH + 'Random_UnitCell_sigma_10.npy'
PATH_XY = PATH + 'Random_UnitCell_XY_10.npy'
PATH_XY_SD = PATH + 'Preprocess_XY_Subdomain.npy'
PATH_SIGMA_SD = PATH + 'Preprocess_sigma_Subdomain.npy'
PATH_SD_MASK = PATH + 'Preprocess_Subdomain_Mask.npy'
PATH_DFUNC = PATH + 'Preprocess_Density_Function.npy'

# Training params
batch_size = 20
learning_rate = 0.001
weight_decay = 1e-4
epochs = 501
# LR scheduler
step_size = 400
gamma = 0.1

# Network params
# modes1 <= width1 // 2 + 1
# modes2 <= width2 // 2 + 1
modes1 = modes2 = 12  # truncated Fourier modes (ky_max, kx_max)
width1 = width2 = 32  # hidden width
n_channels = 32       # hidden channels

# Preprocessing params
# Wether to save or load preprocessing results
SAVE_PREP = False
LOAD_PREP = False
# The number of sub-domains which is 
# generated by KD-Tree splitting algorithm
n_subdomains = 6
# The bandwidth of the kernel of the density estimator
bandwidth = 0.05


################################################################
# Training and evaluation
################################################################
def main(train_xy_sd, train_sigma_sd, train_sd_mask, 
    train_dfunc, test_xy_sd, test_sigma_sd, 
    test_sd_mask, test_dfunc):
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(
            train_xy_sd, train_sigma_sd, 
            train_sd_mask, train_dfunc
        ),
        batch_size=batch_size, shuffle=True,
        generator=torch.Generator(device=device)
    )
    test_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(
            test_xy_sd, test_sigma_sd, 
            test_sd_mask, test_dfunc
        ),
        batch_size=batch_size, shuffle=False,
        generator=torch.Generator(device=device)
    )
    
    model = NUFNO2d(modes1=modes1, modes2=modes2, width1=width1, width2=width2, 
        n_channels=n_channels, n_subdomains=n_subdomains)
    print("Model size: %d"%count_params(model))

    params = list(model.parameters())
    optimizer = Adam(params, lr=learning_rate, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=step_size, gamma=gamma)

    myloss = LpLoss(size_average=False)
    t0 = default_timer()
    for ep in range(epochs):
        model.train()
        t1 = default_timer()
        train_l2 = 0.0
        for xy_sd, sigma, mask, dfunc in train_loader:
            optimizer.zero_grad()
            out = model(dfunc, xy_sd).squeeze(-1) * sigma_std + sigma_mean
            out = out * mask

            loss = myloss(out.view(batch_size, -1), 
                sigma.view(batch_size, -1))
            loss.backward()
            optimizer.step()
            train_l2 += loss.item()

        scheduler.step()

        model.eval()
        test_l2 = 0.0
        with torch.no_grad():
            for xy_sd, sigma, mask, dfunc in test_loader:
                out = model(dfunc, xy_sd).squeeze(-1) * sigma_std + sigma_mean
                out = out * mask

                test_l2 += myloss(out.view(batch_size, -1), 
                    sigma.view(batch_size, -1)).item()

        train_l2 /= ntrain
        test_l2 /= ntest

        t2 = default_timer()
        print("[Epoch {}] Time: {:.1f}s L2: {:>4e} Test_L2: {:>4e}"
                .format(ep, t2-t1, train_l2, test_l2))

    # Return final results
    return train_l2, test_l2, t2-t0


if __name__ == "__main__":
    ################################################################
    # Data loading and preprocessing (via KD-Tree)
    ################################################################
    # Load data
    input_sigma = np.load(PATH_SIGMA)
    input_sigma = np.transpose(input_sigma, (1, 0))
    sigma_mean = np.mean(input_sigma[:ntrain, :])
    sigma_std = np.std(input_sigma[:ntrain, :])
    input_sigma = np.concatenate(
        (input_sigma[:ntrain], input_sigma[-ntest:]), 
        axis=0
    )
    input_xy = np.load(PATH_XY)
    input_xy = np.transpose(input_xy, (2, 0, 1))
    input_xy = np.concatenate(
        (input_xy[:ntrain], input_xy[-ntest:]), 
        axis=0
    )

    if LOAD_PREP:
        input_xy_sd = np.load(PATH_XY_SD)
        input_sigma_sd = np.load(PATH_SIGMA_SD)
        input_sd_mask = np.load(PATH_SD_MASK)
        input_dfunc = np.load(PATH_DFUNC)
    else:
        print("Start KD-Tree splitting...")
        _input_xy_sd = []
        _input_sigma_sd = []
        _input_sd_mask = []
        t1 = default_timer()
        point_cloud = input_xy.reshape(-1, 2).tolist()
        # Use kd-tree to generate subdomain dividings
        tree= KDTree(
            point_cloud, dim=2, n_subdomains=n_subdomains, 
            n_blocks=5
        )
        tree.solve()
        bboxes = tree.get_subdomain_bounding_boxes()
        for i in tqdm(range(ntotal), leave=False):
            point_cloud = input_xy[i]
            # The index (of each point contained in the subdomains) 
            # in the original sequence `point_cloud`
            sd_indices = [
                np.logical_and(
                    np.logical_and(
                        bboxes[j][0][0] <= point_cloud[:, 0],
                        point_cloud[:, 0] <= bboxes[j][0][1]
                    ),
                    np.logical_and(
                        bboxes[j][1][0] <= point_cloud[:, 1],
                        point_cloud[:, 1] <= bboxes[j][1][1]
                    )
                ) for j in range(n_subdomains)
            ]

            sd_n_points = [
                np.sum(sd_indices[j]) for j in range(n_subdomains)
            ]
            max_sd_n_points = np.max(sd_n_points)
            # The point locations in each subdomain
            xy_sd = np.zeros((n_subdomains, max_sd_n_points, 2))
            # The sigma values (at point locations) in each subdomain
            sigma_sd = np.zeros((n_subdomains, max_sd_n_points))
            # The mask for ignoring padded zeros
            sd_mask = np.zeros((n_subdomains, max_sd_n_points))
            for j in range(n_subdomains):
                xy_sd[j, :sd_n_points[j], :] = input_xy[i, sd_indices[j], :]
                sigma_sd[j, :sd_n_points[j]] = input_sigma[i, sd_indices[j]]
                sd_mask[j, :sd_n_points[j]] = 1
            _input_xy_sd.append(xy_sd)
            _input_sigma_sd.append(sigma_sd)
            _input_sd_mask.append(sd_mask)
        
        # Padding
        max_sd_n_points = np.max([sigma_sd.shape[-1] for sigma_sd in _input_sigma_sd])
        input_xy_sd = np.zeros((ntotal, n_subdomains, max_sd_n_points, 2))
        input_sigma_sd = np.zeros((ntotal, n_subdomains, max_sd_n_points))
        input_sd_mask = np.zeros((ntotal, n_subdomains, max_sd_n_points))
        for i in range(ntotal):
            sd_n_points = _input_xy_sd[i].shape[1]
            input_xy_sd[i, :, :sd_n_points, :] = _input_xy_sd[i]
            input_sigma_sd[i, :, :sd_n_points] = _input_sigma_sd[i]
            input_sd_mask[i, :, :sd_n_points] = _input_sd_mask[i]

        t2 = default_timer()
        print("Finish KD-Tree splitting, time elapsed: {:.1f}s".format(t2-t1))

        # Estimate the density distribution (probability) of point clouds, 
        # which serves as the input of the neural operator.
        print("Start estimating the density distribution of point clouds...")
        input_dfunc = []
        t1 = default_timer()
        for i in tqdm(range(ntotal), leave=False):
            kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(input_xy[i])
            # Evaluate KernelDensity
            grid_x = np.linspace(0, 1, num=width2)
            grid_y = np.linspace(0, 1, num=width1)
            grid_x, grid_y = np.meshgrid(grid_x, grid_y)
            # Grid size: grid_size_y x grid_size_x x 2
            # Note: the indexing mode is 'xy' (see np.meshgrid docs for more info)
            grid = np.stack((grid_x, grid_y), axis=-1)
            positions = grid.reshape(-1, 2)
            # Evaluation of KernelDensity
            grid_val = np.exp(kde.score_samples(positions)).reshape(grid.shape[0], grid.shape[1])

            input_dfunc.append(grid_val)
        t2 = default_timer()
        input_dfunc = np.array(input_dfunc)
        print("Finish estimating the density distribution of point clouds, time elapsed: {:.1f}s".format(t2-t1))

        if SAVE_PREP:
            np.save(PATH_XY_SD, input_xy_sd)
            np.save(PATH_SIGMA_SD, input_sigma_sd)
            np.save(PATH_SD_MASK, input_sd_mask)
            np.save(PATH_DFUNC, input_dfunc)

    ################################################################
    # Prepare the dataset
    ################################################################
    input_xy_sd = torch.tensor(input_xy_sd, dtype=torch.float)
        # (ntotal, n_subdomains, max_sd_n_points, 2)
    input_sigma_sd = torch.tensor(input_sigma_sd, dtype=torch.float)
        # (ntotal, n_subdomains, max_sd_n_points)
    input_sd_mask = torch.tensor(input_sd_mask, dtype=torch.float)
        # (ntotal, n_subdomains, max_sd_n_points)
    input_dfunc = torch.tensor(input_dfunc, dtype=torch.float).\
        unsqueeze(-1)
        # (ntotal, width1, width2, 1)

    train_xy = input_xy[:ntrain]
    test_xy = input_xy[-ntest:]
    train_xy_sd = input_xy_sd[:ntrain]
    test_xy_sd = input_xy_sd[-ntest:]
    train_sigma_sd = input_sigma_sd[:ntrain]
    test_sigma_sd = input_sigma_sd[-ntest:]
    train_sd_mask = input_sd_mask[:ntrain]
    test_sd_mask = input_sd_mask[-ntest:]
    train_dfunc = input_dfunc[:ntrain]
    test_dfunc = input_dfunc[-ntest:]

    ################################################################
    # Re-experiment with different random seeds
    ################################################################
    train_l2_res = []
    test_l2_res = []
    time_res = []
    for i in range(5):
        print("=== Round %d ==="%(i+1))
        set_random_seed(SEED_LIST[i])
        train_l2, test_l2, time = main(
            train_xy_sd, train_sigma_sd, train_sd_mask, 
            train_dfunc, test_xy_sd, test_sigma_sd, 
            test_sd_mask, test_dfunc)
        train_l2_res.append(train_l2)
        test_l2_res.append(test_l2)
        time_res.append(time)
    print("=== Finish ===")
    for i in range(5):
        print("[Round {}] Time: {:.1f}s Train_L2: {:>4e} Test_L2: {:>4e}"
                .format(i+1, time_res[i], train_l2_res[i], test_l2_res[i]))
