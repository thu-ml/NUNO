from timeit import default_timer
from tqdm import tqdm
from util.utilities3 import *
from torch.optim import Adam
from ..nufno.nufno_2d import NUFNO2d
from .kdtree import KDTree
from sklearn.neighbors import KernelDensity


# Specify a random seed
# Note: reproducibility is not strictly guaranteed across different hardwares.
# We refer to https://pytorch.org/docs/stable/notes/randomness.html
torch.manual_seed(2021)
np.random.seed(2021)
torch.cuda.manual_seed(2021)
torch.backends.cudnn.deterministic = True


################################################################
# Configs
################################################################

# Data size
Ntotal = 2000
ntrain = 1000
ntest = 200
# Data paths
PATH = 'data/elasticity/'
PATH_SIGMA = PATH + 'Random_UnitCell_sigma_10.npy'
PATH_XY = PATH + 'Random_UnitCell_XY_10.npy'
PATH_IND = PATH + 'Preprocess_ind.npy'
PATH_SEP = PATH + 'Preprocess_sep.npy'
PATH_SD_INFO = PATH + 'Preprocess_subdomain_info.npy'
PATH_DMODES = PATH + 'Preprocess_density_modes.npy'
PATH_DVALS = PATH + 'Preprocess_density_values.npy'

# Training params
batch_size = 20
learning_rate = 0.001
epochs = 501
# Step LR scheduler
step_size = 50
gamma = 0.5

# Network params
# modes1 <= width1 // 2 + 1
# modes2 <= width2 // 2 + 1
modes1 = modes2 = 16  # truncated Fourier modes (ky_max, kx_max)
width1 = width2 = 32  # hidden width
n_channels = 32       # hidden channels

# Preprocessing params
# The number of sub-domains which is 
# generated by KD-Tree splitting algorithm
n_subdomains = 8
# Oversampling ratio for interpolation when preprocessing
oversamp_r_pre = 8
# Oversampling ratio for interpolation when training
oversamp_r_train = 2
# The bandwidth of the kernel of the density estimator
bandwidth = 0.05
# The number of samples for frequency-domain sampling 
# of the density distribution function (point cloud)
# Note: only the top (modes1, modes2) modes will be used in training
N1, N2 = modes1 * 2, modes2 * 2
# Scaling factor of modes of the density function
scale_factor = 2 * modes1 * 2 * modes2

################################################################
# Data loading and preprocessing (via KD-Tree)
################################################################
# Wether to save or load preprocessing results
SAVE_PREP = True
LOAD_PREP = False

# Load data
input_sigma = np.load(PATH_SIGMA)
input_sigma = np.transpose(input_sigma, (1, 0))
input_xy = np.load(PATH_XY)
input_xy = np.transpose(input_xy, (2, 0, 1))
input_xy = np.concatenate(
    (input_xy[:ntrain], input_xy[-ntest:]), 
    axis=0
)

if LOAD_PREP:
    input_ind = np.load(PATH_IND)
    input_sep = np.load(PATH_SEP)
    input_sd_info = np.load(PATH_SD_INFO)
    input_dmodes = np.load(PATH_DMODES)
    input_dvals = np.load(PATH_DVALS)
else:
    print("Start KD-Tree splitting...")
    input_ind = []
    input_sep = []
    input_sd_info = []
    t1 = default_timer()
    for i in tqdm(range(len(input_xy)), leave=False):
        point_cloud = input_xy[i].tolist()  # N_points x 2
        tree= KDTree(
            point_cloud, dim=2, n_subdomains=n_subdomains, 
            n_blocks=8, return_indices=True
        )
        # The index (of each point contained in the subdomains) 
        # in the original sequence `point_cloud`
        subdomain_indices = tree.get_subdomain_indices()
        # The length of `input_ind` is equal to N_points
        input_ind.append(np.concatenate(subdomain_indices))
        # Record the border of the sequence corresponding to 
        # the indices of each subdomain
        # Note: this vector is used to restore `subdomain_indices`
        # from `input_ind`
        subdomain_separator = [0] * (n_subdomains + 1)
        for j in range(n_subdomains):
            subdomain_separator[j+1] = subdomain_separator[j] + \
                subdomain_indices[j].shape[0]
        input_sep.append(subdomain_separator)

        # Gather subdomain info:
        # (xmin, ymin, xlen, ylen, grid_size_x, grid_size_y, n_points)
        bboxes = tree.get_subdomain_bounding_boxes()
        info = []
        for j in range(n_subdomains):
            bbox = bboxes[j]
            n_points = subdomain_indices[j].shape[0]
            # Calculate the grid size used for interpolation when training, 
            # where the length-to-width ratio of the discrete grid 
            # remains the same as the that of the original subdomain ('s bounding box)
            grid_size_x = np.sqrt(n_points * oversamp_r_train * \
                (bbox[0][1] - bbox[0][0]) / (bbox[1][1] - bbox[1][0]))
            grid_size_y = grid_size_x * (bbox[1][1] - bbox[1][0]) / (bbox[0][1] - bbox[0][0])
            grid_size_x, grid_size_y = max(int(np.round(grid_size_x)), 2), max(int(np.round(grid_size_y)), 2)
            # subdomain info: (xmin, ymin, xlen, ylen, grid_size_x, grid_size_y, n_points)
            info.append((bbox[0][0], bbox[1][0], bbox[0][1] - bbox[0][0], bbox[1][1] - bbox[1][0],
                grid_size_x, grid_size_y, len(tree.nodes[j].points)))
        input_sd_info.append(info)
    t2 = default_timer()
    input_ind = np.array(input_ind)
    input_sep = np.array(input_sep)
    input_sd_info = np.array(input_sd_info)
    print("Finish KD-Tree splitting, time elapsed: {:.1f}s".format(t2-t1))

    # Estimate the density distribution (n_points / area or volume) of the point cloud, 
    # which serves as the input of the neural operator.
    # Then compute the Fourier modes of the density function
    print("Start estimating the density distribution of point clouds...")
    # Since the result of real FFT is Hermitian,
    # we can simply store half of it
    input_dvals = []
    input_dmodes = []
    t1 = default_timer()
    for i in tqdm(range(len(input_xy)), leave=False):
        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(input_xy[i])
        # First sample in the global grid
        grid_x = np.linspace(0, 1, num=width1)
        grid_y = np.linspace(0, 1, num=width2)
        grid_x, grid_y = np.meshgrid(grid_x, grid_y)
        grid = np.stack((grid_x, grid_y), axis=-1)
        positions = grid.reshape(-1, 2)
        density_prob = np.exp(kde.score_samples(positions)).reshape(grid.shape[0], grid.shape[1])
        input_dvals.append(density_prob)
        # Then sample in each subdomain
        input_dmodes.append([])
        # Evaluate on each subdomain (uniform grid)
        for j in range(n_subdomains):
            n_points = input_sep[i, j+1] - input_sep[i, j]
            xmin, ymin, xlen, ylen = input_sd_info[i, j, :4]
            # Calculate the grid size used for interpolation when preprocessing, 
            # where the length-to-width ratio of the discrete grid 
            # remains the same as the that of the original subdomain ('s bounding box)
            grid_size_x = np.sqrt(n_points * oversamp_r_pre * xlen / ylen)
            grid_size_y = grid_size_x * ylen / xlen
            grid_size_x, grid_size_y = max(int(np.round(grid_size_x)), 2 * modes2), \
                max(int(np.round(grid_size_y)), 2 * modes1)
            # Generate the uniform grid
            grid_x = np.linspace(xmin, xmin + xlen, num=grid_size_x)
            grid_y = np.linspace(ymin, ymin + ylen, num=grid_size_y)
            grid_x, grid_y = np.meshgrid(grid_x, grid_y)
            # Grid size: grid_size_y x grid_size_x x 2
            # Note: the indexing mode is 'xy' (see np.meshgrid docs for more info)
            grid = np.stack((grid_x, grid_y), axis=-1)
            positions = grid.reshape(-1, 2)
            # Evaluating of KernelDensity
            density_prob = np.exp(kde.score_samples(positions)).reshape(grid.shape[0], grid.shape[1])
            # If the sequence is longer than frequency sample, fold it to N1 x N2
            if density_prob.shape[0] > N1:
                n_paddings1 = N1 - (density_prob.shape[0] % N1)
                density_prob = np.pad(density_prob, (
                    (0, n_paddings1), (0, 0)), 'constant')
                density_prob = density_prob.reshape(-1, N1, density_prob.shape[1])
                density_prob = np.sum(density_prob, axis=0)
            if density_prob.shape[1] > N2:
                n_paddings2 = N2 - (density_prob.shape[1] % N2)
                density_prob = np.pad(density_prob, (
                    (0, 0), (0, n_paddings2)), 'constant')
                density_prob = density_prob.reshape(density_prob.shape[0], -1, N2)
                density_prob = np.sum(density_prob, axis=1)
            # Computing normalized Fourier modes 
            # to facilitate the combination of subdomain modes to form overall domain modes
            modes = np.fft.rfft2(density_prob, s=(N1, N2)) * scale_factor / (grid_size_x * grid_size_y)
            input_dmodes[i].append(np.concatenate((modes[:modes1, :modes2], modes[-modes1:, :modes2]), axis=0))
    
    t2 = default_timer()
    input_dmodes = np.array(input_dmodes)
    input_dvals = np.array(input_dvals)
    print("Finish estimating the density distribution of point clouds, time elapsed: {:.1f}s".format(t2-t1))

    if SAVE_PREP:
        np.save(PATH_IND, input_ind)
        np.save(PATH_SEP, input_sep)
        np.save(PATH_SD_INFO, input_sd_info)
        np.save(PATH_DMODES, input_dmodes)
        np.save(PATH_DVALS, input_dvals)

################################################################
# Preparing the dataset
################################################################
input_xy = torch.tensor(input_xy, dtype=torch.float)
input_sigma = torch.tensor(input_sigma, dtype=torch.float).unsqueeze(-1)
input_ind = torch.tensor(input_ind, dtype=torch.long)
input_sep = torch.tensor(input_sep, dtype=torch.long)
input_sd_info = torch.tensor(input_sd_info)
input_dmodes = torch.tensor(input_dmodes, dtype=torch.cfloat).unsqueeze(-1)
input_dvals = torch.tensor(input_dvals).unsqueeze(-1)

train_xy = input_xy[:ntrain]
test_xy = input_xy[-ntest:]
train_sigma = input_sigma[:ntrain]
test_sigma = input_sigma[-ntest:]
train_ind = input_ind[:ntrain]
test_ind = input_ind[-ntest:]
train_sep = input_sep[:ntrain]
test_sep = input_sep[-ntest:]
train_sd_info = input_sd_info[:ntrain]
test_sd_info = input_sd_info[-ntest:]
train_dmodes = input_dmodes[:ntrain]
test_dmodes = input_dmodes[-ntest:]
train_dvals = input_dvals[:ntrain]
test_dvals = input_dvals[-ntest:]

train_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(
        train_xy, train_sigma, 
        train_ind, train_sep, train_sd_info,
        train_dmodes, train_dvals
    ), 
    batch_size=batch_size, shuffle=True,
    generator=torch.Generator(device=device)
)
test_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(
        test_xy, test_sigma,
        test_ind, test_sep, test_sd_info,
        test_dmodes, test_dvals
    ), 
    batch_size=batch_size, shuffle=False,
    generator=torch.Generator(device=device)
)

################################################################
# Training and evaluation
################################################################
model = NUFNO2d(modes1=modes1, modes2=modes2, width1=width1, width2=width2, 
    n_channels_global=n_channels, n_channels_local=n_channels//4, n_subdomains=n_subdomains)
print("Model size: %d"%count_params(model))

params = list(model.parameters())
optimizer = Adam(params, lr=learning_rate, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

myloss = LpLoss(size_average=False)
for ep in range(epochs):
    model.train()
    t1 = default_timer()
    train_l2 = 0.0
    for xy, sigma, ind, sep, sd_info, dmodes, dvals in train_loader:
        optimizer.zero_grad()
        out = model(dmodes, dvals, xy, ind, sep, sd_info)

        loss = myloss(out.view(batch_size, -1), sigma.view(batch_size, -1))
        loss.backward()
        optimizer.step()
        train_l2 += loss.item()

    scheduler.step()

    model.eval()
    test_l2 = 0.0
    with torch.no_grad():
        for xy, sigma, ind, sep, sd_info, dmodes, dvals in test_loader:
            out = model(dmodes, dvals, xy, ind, sep, sd_info)
            test_l2 += myloss(out.view(batch_size, -1), sigma.view(batch_size, -1)).item()

    train_l2 /= ntrain
    test_l2 /= ntest

    t2 = default_timer()
    print("[Epoch {}] Time: {:.1f}s L2: {:>4e} Test_L2: {:>4e}"
            .format(ep, t2-t1, train_l2, test_l2))

    if ep%100==0:
        pass
        # XY = loc[-1].squeeze().detach().cpu().numpy()
        # truth = sigma[-1].squeeze().detach().cpu().numpy()
        # pred = out[-1].squeeze().detach().cpu().numpy()

        # lims = dict(cmap='RdBu_r', vmin=truth.min(), vmax=truth.max())
        # fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))
        # ax[0].scatter(XY[:, 0], XY[:, 1], 100, truth, edgecolor='w', lw=0.1, **lims)
        # ax[1].scatter(XY[:, 0], XY[:, 1], 100, pred, edgecolor='w', lw=0.1, **lims)
        # ax[2].scatter(XY[:, 0], XY[:, 1], 100, truth - pred, edgecolor='w', lw=0.1, **lims)
        # fig.show()