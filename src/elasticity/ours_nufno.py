from timeit import default_timer
from tqdm import tqdm
from util.utilities3 import *
from torch.optim import Adam
from ..nufno.nufno_2d import NUFNO2d
from .kdtree import KDTree
from sklearn.neighbors import KernelDensity


# Specify a random seed
# Note: reproducibility is not strictly guaranteed across different hardwares.
# We refer to https://pytorch.org/docs/stable/notes/randomness.html
torch.manual_seed(2021)
np.random.seed(2021)
torch.cuda.manual_seed(2021)
torch.backends.cudnn.deterministic = True


################################################################
# Configs
################################################################

# Data size
Ntotal = 2000
ntrain = 1000
ntest = 200
# Data paths
PATH = 'data/elasticity/'
PATH_SIGMA = PATH + 'Random_UnitCell_sigma_10.npy'
PATH_XY = PATH + 'Random_UnitCell_XY_10.npy'
PATH_SD_INFO = PATH + 'Preprocess_subdomain_info.npy'
PATH_DFUNC_SD = PATH + 'Preprocess_density_function_subdomain.npy'
PATH_DFUNC_G = PATH + 'Preprocess_density_function_global.npy'

# Training params
batch_size = 20
learning_rate = 0.001
epochs = 501
# Step LR scheduler
step_size = 50
gamma = 0.5

# Network params
# modes1 <= width1 // 2 + 1
# modes2 <= width2 // 2 + 1
modes1 = modes2 = 16  # truncated Fourier modes (ky_max, kx_max)
width1 = width2 = 32  # hidden width
n_channels = 32       # hidden channels

# Preprocessing params
# The number of sub-domains which is 
# generated by KD-Tree splitting algorithm
n_subdomains = 8
# Oversampling ratio (>=1) for preprocessing interpolation
oversamp_r1 = oversamp_r2 = 3
# The bandwidth of the kernel of the density estimator
bandwidth = 0.05
# The number of samples for frequency-domain sampling 
# of the density distribution function (point cloud)
# Note: only the top (modes1, modes2) modes will be used in training
N1, N2 = modes1 * 2, modes2 * 2
# L1 >= N1, L2 >= N2
L1, L2 = modes1 * 2, modes2 * 2
# Scaling factor of modes of the density function
scale_factor = 2 * modes1 * 2 * modes2

################################################################
# Data loading and preprocessing (via KD-Tree)
################################################################
# Wether to save or load preprocessing results
SAVE_PREP = False
LOAD_PREP = True

# Load data
input_sigma = np.load(PATH_SIGMA)
input_sigma = np.transpose(input_sigma, (1, 0))
input_xy = np.load(PATH_XY)
input_xy = np.transpose(input_xy, (2, 0, 1))
input_xy = np.concatenate(
    (input_xy[:ntrain], input_xy[-ntest:]), 
    axis=0
)

if LOAD_PREP:
    input_sd_info = np.load(PATH_SD_INFO)
    input_dfunc_sd = np.load(PATH_DFUNC_SD)
    input_dfunc_g = np.load(PATH_DFUNC_G)
else:
    print("Start KD-Tree splitting...")
    input_sd_info = []
    t1 = default_timer()
    for i in tqdm(range(len(input_xy)), leave=False):
        point_cloud = input_xy[i].tolist()  # N_points x 2
        # Use kd-tree to generate subdomain dividings
        tree= KDTree(
            point_cloud, dim=2, n_subdomains=n_subdomains, 
            n_blocks=8, overall_borders=[(0, 1), (0, 1)]
        )
        tree.solve()
        # Gather subdomain info:
        # (xmin, ymin, xlen, ylen)
        borders = tree.get_subdomain_borders(shrink=True)
        info = []
        for j in range(n_subdomains):
            border = borders[j]
            # subdomain info: (xmin, ymin, xlen, ylen)
            info.append((border[0][0], border[1][0], 
                border[0][1] - border[0][0], border[1][1] - border[1][0]))
        input_sd_info.append(info)
    t2 = default_timer()
    input_sd_info = np.array(input_sd_info)
    print("Finish KD-Tree splitting, time elapsed: {:.1f}s".format(t2-t1))

    # Estimate the density distribution (n_points / area or volume) of the point cloud, 
    # which serves as the input of the neural operator.
    # Then compute the Fourier modes of the density function
    print("Start estimating the density distribution of point clouds...")
    # Since the result of real FFT is Hermitian,
    # we can simply store half of it
    input_dfunc_sd = []
    input_dfunc_g = []
    t1 = default_timer()
    for i in tqdm(range(len(input_xy)), leave=False):
        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(input_xy[i])
        # First sample in the global grid
        grid_x = np.linspace(0, 1, num=width2)
        grid_y = np.linspace(0, 1, num=width1)
        grid_x, grid_y = np.meshgrid(grid_x, grid_y)
        grid = np.stack((grid_x, grid_y), axis=-1)
        positions = grid.reshape(-1, 2)
        density_prob = np.exp(kde.score_samples(positions)).reshape(grid.shape[0], grid.shape[1])
        input_dfunc_g.append(density_prob)
        # Then sample in each subdomain
        input_dfunc_sd.append([])
        # Evaluate on each subdomain (uniform grid)
        for j in range(n_subdomains):
            xmin, ymin, xlen, ylen = input_sd_info[i, j, :4]
            # Generate the uniform grid
            L1_sd = max(int(np.round(ylen * L1 * oversamp_r1)), 2)
            L2_sd = max(int(np.round(xlen * L2 * oversamp_r2)), 2)
            N1_sd = int(np.round(N1 * oversamp_r1))
            N2_sd = int(np.round(N2 * oversamp_r2))
            grid_x = np.linspace(xmin, xmin + xlen, num=L2_sd)
            grid_y = np.linspace(ymin, ymin + ylen, num=L1_sd)
            grid_x, grid_y = np.meshgrid(grid_x, grid_y)
            # Grid size: grid_size_y x grid_size_x x 2
            # Note: the indexing mode is 'xy' (see np.meshgrid docs for more info)
            grid = np.stack((grid_x, grid_y), axis=-1)
            positions = grid.reshape(-1, 2)
            # Evaluating of KernelDensity
            density_prob = np.exp(kde.score_samples(positions)).reshape(grid.shape[0], grid.shape[1])
            modes = np.fft.rfft2(density_prob, s=(N1_sd, N2_sd))
            modes = np.concatenate((modes[:modes1, :modes2], modes[-modes1:, :modes2]), axis=0)
            input_dfunc_sd[i].append(
                modes / (oversamp_r1 * oversamp_r2)
            )
    t2 = default_timer()
    input_dfunc_sd = np.array(input_dfunc_sd)
    input_dfunc_g = np.array(input_dfunc_g)
    print("Finish estimating the density distribution of point clouds, time elapsed: {:.1f}s".format(t2-t1))

    if SAVE_PREP:
        np.save(PATH_SD_INFO, input_sd_info)
        np.save(PATH_DFUNC_SD, input_dfunc_sd)
        np.save(PATH_DFUNC_G, input_dfunc_g)

################################################################
# Preparing the dataset
################################################################
input_xy = torch.tensor(input_xy, dtype=torch.float)
input_sigma = torch.tensor(input_sigma, dtype=torch.float).unsqueeze(-1)
input_sd_info = torch.tensor(input_sd_info, dtype=torch.float)
input_dfunc_sd = torch.tensor(input_dfunc_sd, dtype=torch.float).unsqueeze(-1)
input_dfunc_g = torch.tensor(input_dfunc_g, dtype=torch.float).unsqueeze(-1)

train_xy = input_xy[:ntrain]
test_xy = input_xy[-ntest:]
train_sigma = input_sigma[:ntrain]
test_sigma = input_sigma[-ntest:]
train_sd_info = input_sd_info[:ntrain]
test_sd_info = input_sd_info[-ntest:]
train_dfunc_sd = input_dfunc_sd[:ntrain]
test_dfunc_sd = input_dfunc_sd[-ntest:]
train_dfunc_g = input_dfunc_g[:ntrain]
test_dfunc_g = input_dfunc_g[-ntest:]

train_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(
        train_xy, train_sigma, 
        train_sd_info, train_dfunc_sd, train_dfunc_g
    ), 
    batch_size=batch_size, shuffle=True,
    generator=torch.Generator(device=device)
)
test_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(
        test_xy, test_sigma,
        test_sd_info, test_dfunc_sd, test_dfunc_g
    ), 
    batch_size=batch_size, shuffle=False,
    generator=torch.Generator(device=device)
)

################################################################
# Training and evaluation
################################################################
model = NUFNO2d(modes1=modes1, modes2=modes2, width1=width1, width2=width2, 
    n_channels=n_channels, n_subdomains=n_subdomains)
print("Model size: %d"%count_params(model))

params = list(model.parameters())
optimizer = Adam(params, lr=learning_rate, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

myloss = LpLoss(size_average=False)
for ep in range(epochs):
    model.train()
    t1 = default_timer()
    train_l2 = 0.0
    for xy, sigma, sd_info, dfunc_sd, dfunc_g in train_loader:
        optimizer.zero_grad()
        out = model(dfunc_sd, sd_info, xy)

        loss = myloss(out.view(batch_size, -1), sigma.view(batch_size, -1))
        loss.backward()
        optimizer.step()
        train_l2 += loss.item()

    scheduler.step()

    model.eval()
    test_l2 = 0.0
    with torch.no_grad():
        for xy, sigma, sd_info, dfunc_sd, dfunc_g in test_loader:
            out = model(dfunc_sd, sd_info, xy)
            test_l2 += myloss(out.view(batch_size, -1), sigma.view(batch_size, -1)).item()

    train_l2 /= ntrain
    test_l2 /= ntest

    t2 = default_timer()
    print("[Epoch {}] Time: {:.1f}s L2: {:>4e} Test_L2: {:>4e}"
            .format(ep, t2-t1, train_l2, test_l2))

    if ep%100==0:
        pass
        # XY = loc[-1].squeeze().detach().cpu().numpy()
        # truth = sigma[-1].squeeze().detach().cpu().numpy()
        # pred = out[-1].squeeze().detach().cpu().numpy()

        # lims = dict(cmap='RdBu_r', vmin=truth.min(), vmax=truth.max())
        # fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))
        # ax[0].scatter(XY[:, 0], XY[:, 1], 100, truth, edgecolor='w', lw=0.1, **lims)
        # ax[1].scatter(XY[:, 0], XY[:, 1], 100, pred, edgecolor='w', lw=0.1, **lims)
        # ax[2].scatter(XY[:, 0], XY[:, 1], 100, truth - pred, edgecolor='w', lw=0.1, **lims)
        # fig.show()